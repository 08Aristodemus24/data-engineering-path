{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articles, Videos, Papers, Repositories:\n",
    "* SQL window functions: https://www.youtube.com/watch?v=Ww71knvhQ-s&pp=ygUUc3FsIHdpbmRvdyBmdW5jdGlvbnM%3D\n",
    "* Projects to emulate:\n",
    "- https://www.startdataengineering.com/post/data-engineering-project-for-beginners-batch-edition/#1-introduction\n",
    "- https://www.ssp.sh/blog/data-engineering-project-in-twenty-minutes/\n",
    "* start DE free cousrse: https://github.com/josephmachado/sde_de101_josephmachado/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights:\n",
    "* Data orchestration uses apache airflow which schedules jobs at a specific cadence for tasks like data ingestion, processing, and transformation\n",
    "* Skim through these documentation quickly to get a high level overview\n",
    "AWS S3: https://aws.amazon.com/s3/\n",
    "PostgreSQL: https://www.postgresql.org/docs/\n",
    "Airflow: https://airow.apache.org/docs/apache-airowtable/core-concepts/index.html\n",
    "Project idea: Create an Airow DAG that extracts data from your favorite source (nance, sports, music) and\n",
    "uploads it to a Data Warehouse (any relational database of your choice)\n",
    "\n",
    "* Batch processing is just a toolframework to manage growing data and enable us to process them in a scalable manner. Scalable means simply that: a software can handle increased workloads, such as more users or larger datasets, while maintaining performance, a sofware can easily add or remove resources to meet changing demands, and a software remains stable and maintains performance even after a sudden increase in workload. \n",
    "\n",
    "* So far I see from the data engineering project template from Joseph Machado that make files are common which maybe house commands to run docker files\n",
    "```\n",
    "####################################################################################################################\n",
    "# Setup containers to run Airflow\n",
    "\n",
    "docker-spin-up:\n",
    "\tdocker compose build && docker compose up airflow-init && docker compose up --build -d \n",
    "\n",
    "perms:\n",
    "\tsudo mkdir -p logs plugins temp dags tests data visualization && sudo chmod -R u=rwx,g=rwx,o=rwx logs plugins temp dags tests data visualization\n",
    "\n",
    "setup-conn:\n",
    "\tdocker exec scheduler python /opt/airflow/setup_conn.py\n",
    "\n",
    "do-sleep:\n",
    "\tsleep 30\n",
    "\n",
    "up: perms docker-spin-up do-sleep setup-conn\n",
    "\n",
    "down:\n",
    "\tdocker compose down\n",
    "\n",
    "restart: down up\n",
    "\n",
    "sh:\n",
    "\tdocker exec -ti webserver bash\n",
    "\n",
    "####################################################################################################################\n",
    "# Testing, auto formatting, type checks, & Lint checks\n",
    "pytest:\n",
    "\tdocker exec webserver pytest -p no:warnings -v /opt/airflow/tests\n",
    "\n",
    "format:\n",
    "\tdocker exec webserver python -m black -S --line-length 79 .\n",
    "\n",
    "isort:\n",
    "\tdocker exec webserver isort .\n",
    "\n",
    "type:\n",
    "\tdocker exec webserver mypy --ignore-missing-imports /opt/airflow\n",
    "\n",
    "lint: \n",
    "\tdocker exec webserver flake8 /opt/airflow/dags\n",
    "\n",
    "ci: isort format type lint pytest\n",
    "```\n",
    "\n",
    "running `make up` in the directory that contains this `Makefile` will somehow run the command under the `up` statement idk if that's how you say it but under `up:` it which is `perms docker-spin-up do-sleep setup-conn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
